# Requirements: Sample HTML Collector for Tax Lien Data

**Date:** 2025-12-31
**Phase:** REQUIREMENTS
**Purpose:** –°–æ–±—Ä–∞—Ç—å –æ–±—Ä–∞–∑—Ü—ã HTML-—Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ –≤—Å–µ—Ö county tax websites –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø–∞—Ä—Å–µ—Ä–æ–≤

---

## üìä Problem Statement

### Current Situation

**Existing Scraper:**
- –£–∂–µ –µ—Å—Ç—å production scraper –≤ `parser/taxlien-scraper-python/`
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 4 –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã: QPublic, Beacon, Tyler Technologies, Bid4Assets
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Celery + SeleniumBase –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç HTML –≤ `downloaded_files/`

**Problem:**
- –ü–µ—Ä–µ–¥ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –Ω–æ–≤–æ–π –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –Ω—É–∂–Ω—ã HTML-–ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
- –†—É—á–Ω–æ–π —Å–±–æ—Ä –æ–±—Ä–∞–∑—Ü–æ–≤ –º–µ–¥–ª–µ–Ω–Ω—ã–π –∏ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π
- –ù–µ—Ç —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –±–∞–∑—ã –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–æ–≤
- –°–ª–æ–∂–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ä—Å–µ—Ä –±–µ–∑ –∂–∏–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

### What We Need

**Sample Collector Tool** - –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–±–æ—Ä–∞ HTML-–æ–±—Ä–∞–∑—Ü–æ–≤ —Å–æ –≤—Å–µ—Ö county websites:

1. **Input:** CSV file —Å 130+ –æ–∫—Ä—É–≥–∞–º–∏ (`Tax Liens - Sources - Sheet1.csv`)
2. **Output:** –û—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è HTML-—Ñ–∞–π–ª–æ–≤ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ:
   ```
   samples_collected/
   ‚îú‚îÄ‚îÄ fl_/
   ‚îÇ   ‚îú‚îÄ‚îÄ columbia/
   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ custom_gis/
   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ assessor_R00010-001.html
   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ assessor_R00010-001_meta.json
   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tax_R00010-001.html
   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gis_142S1500061000.html
   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadata.json
   ‚îÇ   ‚îú‚îÄ‚îÄ union/
   ‚îÇ   ‚îî‚îÄ‚îÄ ...
   ‚îú‚îÄ‚îÄ az_/
   ‚îî‚îÄ‚îÄ ...
   ```
3. **Coverage:** 108 –æ–±—Ä–∞–∑—Ü–æ–≤ –Ω–∞ –æ–∫—Ä—É–≥ √ó 130 –æ–∫—Ä—É–≥–æ–≤ = ~14,040 HTML —Ñ–∞–π–ª–æ–≤

---

## üë• User Stories

### Story 1: Developer Testing Parser
**As a** parser developer
**I want** to have 3-5 real HTML examples from each county
**So that** I can test my parser logic without making live requests

**Acceptance Criteria:**
- ‚úÖ Each county has at least 3 HTML samples
- ‚úÖ Samples include different types: assessor, tax, GIS, recorder
- ‚úÖ Metadata includes URL, parcel ID, download date
- ‚úÖ Screenshots saved for JavaScript-heavy pages

---

### Story 2: Platform Research
**As a** platform analyst
**I want** to download samples from a new platform
**So that** I can study its HTML structure before implementing a parser

**Acceptance Criteria:**
- ‚úÖ Can specify specific counties to download
- ‚úÖ Can use Selenium for JavaScript-heavy pages
- ‚úÖ HTML is saved with proper encoding (UTF-8)
- ‚úÖ Failed downloads are logged with error details

---

### Story 3: Regression Testing
**As a** QA engineer
**I want** to have a static collection of HTML samples
**So that** I can run parser tests without depending on live websites

**Acceptance Criteria:**
- ‚úÖ Samples are version-controlled
- ‚úÖ Each sample has metadata (download date, platform type)
- ‚úÖ Can re-download samples to compare HTML changes over time
- ‚úÖ Summary report shows coverage stats

---

## üéØ Functional Requirements

### FR1: Source Management
**Priority:** HIGH

**Description:** –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥ CSV —Ñ–∞–π–ª–∞ —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –æ–∫—Ä—É–≥–æ–≤

**Requirements:**
- Read `Tax Liens - Sources - Sheet1.csv` (130 counties)
- Parse columns: state, county, Assessor URL, Tax URL, GIS URL, Recorder URL
- Identify platform type from URL patterns and indicators (QP, PT, GIS, etc.)
- Filter counties by state or platform type

**Inputs:**
- CSV file path
- Optional: state filter (e.g., `fl_`, `az_`)
- Optional: platform filter (e.g., `qpublic`, `custom_gis`)

**Outputs:**
- List of counties to process
- Platform identification for each county

---

### FR2: URL Generation
**Priority:** HIGH

**Description:** –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∞–±–æ—á–∏—Ö URL –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –æ–±—Ä–∞–∑—Ü–æ–≤

**Requirements:**
- Use working example URLs from CSV (example parcel IDs)
- Generate 3-5 URLs per county for different page types
- Support platform-specific URL patterns:
  - **Custom GIS (floridapa.com):** `?pin=PARCEL_ID`
  - **QPublic:** Search-based, needs interactive navigation
  - **PropertyTax:** `?parcel=PARCEL_ID`
  - **Tyler:** ASPX ViewState-based

**Inputs:**
- County config (state, county, platform)
- Sample parcel IDs

**Outputs:**
- List of `SampleURL` objects with:
  - url
  - parcel_id
  - page_type (assessor, tax, gis, recorder)
  - platform
  - notes

---

### FR3: HTML Download
**Priority:** HIGH

**Description:** –°–∫–∞—á–∏–≤–∞–Ω–∏–µ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü —Å —Ä–∞–∑–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏

**Methods:**
1. **Simple HTTP** - –¥–ª—è —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
2. **Selenium** - –¥–ª—è JavaScript-heavy —Å—Ç—Ä–∞–Ω–∏—Ü

**Requirements:**
- Auto-detect which method to use based on platform
- Save HTML with UTF-8 encoding
- Save screenshots for Selenium downloads
- Polite delays between requests (2-4 seconds)
- User-Agent rotation
- Handle redirects and errors gracefully

**Inputs:**
- SampleURL object
- Method choice (auto/simple/selenium)

**Outputs:**
- HTML file: `{page_type}_{parcel_id}.html`
- Screenshot: `{page_type}_{parcel_id}.png` (if Selenium)
- Metadata JSON: `{page_type}_{parcel_id}_meta.json`

---

### FR4: Metadata Tracking
**Priority:** MEDIUM

**Description:** –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –æ –∫–∞–∂–¥–æ–º –æ–±—Ä–∞–∑—Ü–µ

**Metadata Fields:**
- url: Original URL
- parcel_id: Parcel identifier
- page_type: assessor/tax/gis/recorder
- platform: Platform type
- county: County name
- state: State code
- download_date: ISO timestamp
- method: simple_http/selenium
- status_code: HTTP status (if applicable)
- content_length: HTML size in bytes
- notes: Any special notes

**Output Format:** JSON

---

### FR5: Progress Tracking & Reporting
**Priority:** MEDIUM

**Description:** –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞

**Requirements:**
- Show progress: `[15/130] FL Columbia - downloading 3 URLs...`
- Track stats:
  - Total downloads attempted
  - Successful downloads
  - Failed downloads
  - By platform breakdown
  - By state breakdown
- Generate summary report: `collection_summary.json`

**Report Fields:**
```json
{
  "collection_date": "2025-12-31T...",
  "stats": {
    "total_downloads": 450,
    "successful": 412,
    "failed": 38,
    "by_platform": {
      "custom_gis": 87,
      "qpublic": 126,
      "propertytax": 98,
      ...
    },
    "by_state": {
      "fl_": 305,
      "az_": 67,
      "new jersey": 12,
      "new mexico": 28
    }
  },
  "results": [...]
}
```

---

## üö´ Non-Functional Requirements

### NFR1: Performance
- **Download Speed:** 2-4 seconds delay per request (polite scraping)
- **Batch Processing:** Can process all 130 counties in ~2-4 hours
- **Memory:** < 2GB RAM usage
- **Disk Space:** ~500MB for 500 samples (avg 1MB per HTML)

### NFR2: Reliability
- **Error Handling:** Graceful handling of HTTP errors, timeouts
- **Resume Support:** Can resume from where it left off
- **Duplicate Prevention:** Skip already downloaded samples
- **Logging:** Detailed logs for debugging

### NFR3: Usability
- **CLI Interface:** Simple command-line arguments
- **Filters:** Can filter by state, county, platform
- **List Mode:** `--list` to show all available counties
- **Dry Run:** `--dry-run` to preview what will be downloaded

### NFR4: Maintainability
- **Modular Code:** Separate concerns (URL generation, download, metadata)
- **Configuration:** Platform-specific configs in separate file
- **Documentation:** Clear README with examples

---

## üîó Dependencies

### External Dependencies
- **Python 3.13+**
- **SeleniumBase** - for JavaScript-heavy pages
- **requests** - for simple HTTP downloads
- **BeautifulSoup4** - for HTML parsing (metadata extraction)
- **sbvirtualdisplay** - for headless Selenium

### Internal Dependencies
- **Tax Liens - Sources - Sheet1.csv** - must exist
- **Existing scraper** (`parser/taxlien-scraper-python/`) - –¥–ª—è reference, –Ω–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å

---

## üì¶ Deliverables

### Scripts
1. **platform_sample_urls.py** ‚úÖ Done
   - `PlatformURLGenerator` class
   - Working example URLs for 14 counties

2. **download_samples.py** ‚úÖ Done
   - `SampleDownloader` class
   - CLI interface
   - Progress tracking

3. **sample_collector.py** ‚úÖ Done
   - Main orchestrator
   - Combines URL generation + download

### Documentation
4. **README.md**
   - Quick start guide
   - Usage examples
   - Troubleshooting

### Data
5. **samples_collected/** directory
   - Organized by state/county/platform
   - ~400-650 HTML files
   - Metadata JSON files

6. **collection_summary.json**
   - Statistics report
   - Coverage metrics

---

## üéØ Success Metrics

### Coverage Metrics
- ‚úÖ **130 counties cataloged** from CSV
- ‚úÖ **14 counties with working example URLs** (FL: 12, AZ: 2)
- üéØ **50+ counties with downloaded samples** (target)
- üéØ **300+ HTML samples collected** (3 per county √ó 100 counties)

### Quality Metrics
- üéØ **95%+ download success rate** for counties with working URLs
- üéØ **100% metadata completeness** for successful downloads
- üéØ **All platforms covered:** QPublic, Custom GIS, PropertyTax, Tyler, etc.

### Platform Coverage
| Platform | Counties in CSV | Working URLs | Target Samples |
|----------|----------------|--------------|----------------|
| QPublic (QP) | 27 | 6 | 18-30 |
| PropertyTax (PT) | 28 | 0 | 0-15 |
| Custom GIS | 6 | 6 | 18-30 |
| MyFloridaCounty (MF) | 17 | 3 | 9-15 |
| GovernMax (GM) | 3 | 3 | 9-15 |
| Tyler | ? | 2 | 6-10 |

---

## üöÄ Out of Scope

This tool is **NOT**:
- ‚ùå A production scraper (that's `taxlien-scraper-python/`)
- ‚ùå A parser (that's `platforms/*/parse_html()`)
- ‚ùå A database loader (that's MongoDB/PostgreSQL integration)
- ‚ùå A scheduler (that's Celery)

This tool **IS**:
- ‚úÖ A one-time sample collection utility
- ‚úÖ A testing data generator
- ‚úÖ A platform research tool

---

## üîÑ Integration with Existing System

### Relationship to `taxlien-scraper-python/`

**Sample Collector** (this SDD):
- Purpose: Collect test data
- Scope: 3-5 samples per county
- Frequency: One-time or periodic updates
- Output: `samples_collected/` directory

**Production Scraper** (`taxlien-scraper-python/`):
- Purpose: Continuous data collection
- Scope: All parcels in county (thousands)
- Frequency: Daily/weekly via Celery beat
- Output: MongoDB (raw HTML) + PostgreSQL (parsed data)

**Workflow:**
```
1. Use Sample Collector to get HTML examples
     ‚Üì
2. Develop parser using samples (offline testing)
     ‚Üì
3. Add parser to Production Scraper
     ‚Üì
4. Deploy Production Scraper with Celery
```

---

## ‚úÖ Requirements Approval Checklist

Before moving to SPECIFICATIONS phase:

- [x] User stories defined with acceptance criteria
- [x] Functional requirements documented (FR1-FR5)
- [x] Non-functional requirements defined (NFR1-NFR4)
- [x] Dependencies identified
- [x] Success metrics established
- [x] Out of scope clearly defined
- [x] Integration with existing system explained
- [ ] **User approval:** Requirements reviewed and approved

---

**Status:** READY FOR REVIEW
**Next Phase:** SPECIFICATIONS
**Blocker:** None

---

## üó∫Ô∏è Related SDDs

- **[sdd-scraper-service](../sdd-scraper-service/)** - Main scraper service with 15 platform support
  - This SDD focuses on production scraping (ALL parcels)
  - Sample collector focuses on TEST DATA (3-5 samples per county)

- **[sdd-data-structure](../sdd-data-structure/)** - Database schemas
  - Sample collector saves raw HTML (no DB integration)
  - Production scraper saves to MongoDB + PostgreSQL

- **[sdd-data-pipeline](../sdd-data-pipeline/)** - ETL workflow
  - Sample collector is NOT part of ETL pipeline
  - It's a development tool for parser testing
